Log de trabalho

- Da escolha do Scrapy como framework: 

Por já possuir experiencia com o scrapy achei mais fácil utilizá-lo. 
Scrapy dá liberdade para que você crie itens, pipelines, middlwares e vários outros arquivos que podem auxiliar na hora do scraping.

- Da escolha do mongo como banco de dados:

Em resumo, a escolha se deu pela facilidade em inserir, acessar e estruturar os dados. 

- Da lógica das spiders:

Primeiro escolhi os sites e investiguei que tipos de campos poderiam ser gerados a partir dos dados 
encontrados na página, e como eles poderiam ser estruturados em relação ao código.
Alguns campos diferem de um site para o outros, por exemplo, o tecmundo sugere um tempo de leitura para a matéria, 
o tecblog não, no tecblog as materias podem ter sub titulo, no tecmundo não. Mas isso já era esperado.
Tentei então, extrair todos os dados possíveis levando em consideração cada site. 
Optei por fazer a limpeza de parte dos dados que poderiam facilmente ser tratados utilizando o próprio scrapy. 
O scrapy permite que você utilize processors que podem ser inseridos na saída/entrada de valores no loader do seu item. 
No projeto isso foi utilizado por exemplo, para limpar as tags, que na maioria das vezes,
eram extraídas das páginas com espaço entre o começo e o final da string.
E também, para salvar a data da publicação em um formato datetime, pois para futuras consultas não seria necessário fazer uma conversão. 
Escolhi também modularizar a spider, pois mantendo as funções separadas e, 
tentando utilizar o framework da melhor forma possível é mais fácil de entender o que o projeto se encarrega de fazer.

Por isso nas pastas das spiders é possível encontrar os arquivos:

- constants: com as urls de consulta e xpaths dos campos.
- spiders: com a classe da spider, iniciando as requisições e chamando a função de extrair os dados do site.
- steps: contendo a lista de passos para a extração dos dados de cada spider.

Na classe do loader do item duas novas funções foram criadas para auxiliar no carregamento dos xpaths e dos values.
O motivo foi para evitar que o código ficasse repetitivo e complicado de entender. A lógica foi utilizar as funções que
já exisitiam add_xpath e add_value e adaptá-las para receber um dicionário contendo os valores e por fim, adicionar os valores normalmente.
Na extração do texto da materia foi utilizado beautifulsoup.

- Sobre testes e coverage:

Pytest é um ótimo framework para testes, e todo mundo sabe que um projeto sem testes é um projeto com um pé no degrau de vários bugs.
Fiz dois testes para funções criadas em processors. É possivel também utilizando pytest-cov visualizar a cobertura de testes de cada arquivo do projeto.
O que é muito util para saber quais arquivos precisam de testes. 
É possivel utilizar essas informações em outras ferramentas e apis, como por exemplo: python-coveralls [https://github.com/z4r/python-coveralls] 
onde você pode manter um histórico de cobertura do seu código. 

- Sobre a execução do projeto:

Tentei fazer com que não fosse muito díficil de executar o projeto, por isso escolhi usar .env 
para poder escolher a quantidade de páginas a serem extraidas de cada site. 
E o uso de um entrypoint para a execução, atrelado ao Makefile com comandos auxiliares. 


- Formas de escalar o projeto:

Não é muito dificil escalar um projeto utilizando scrapy, é possível utilizar o scrapyd [https://scrapyd.readthedocs.io/en/stable/] 
que dispõe de uma api json que permite você faça schedule de extrações passando o payload contendo os argumentos da spider. 
É possível também utilizar o docker e fazer com que as instancias do scrapyd rodem nas imagens.
O scraping hub também oferece várias soluções para permitir que projetos utilizando scrapy sejam escalaveis. [https://scrapinghub.com/]

Nesse projeto não houve problemas como: javascripts gerando dados da página que eram essenciais, bloqueios de ip.
Mas esses problemas e vários outros podem ser resolvidos utilizando ferramentas do proprio scraping hub, como:

- Crawlera que por meio de proxy faz com que exista um controle de requisições por ip, evitando que a spider seja "banida"
de algum site. 
- Splash que permite que scripts em páginas sejam executados, caso haja problemas com cookies ou outras coisas.
- Scrapy cloud que é uma plataforma em nuvem para executar spiders.

- Deploy

Para o deploy o travis poderia ser utilizado, por ser fácil de integrar e de visualizar quando algo do código fez algum impacto negativo no projeto. 
Em produção o sentry poderia ser utilizado para monitorar erros futuros. (As vezes xpaths mudam, urls ou até mesmo a página inteira, isso pode "quebrar a spider")


- Sobre analise de dados:

Executei algumas analises utilizando pandas e matplotlib, alem das analises feitas
varias outras poderiam ter sido aplicadas utilizando processamento de linguagem natural.
